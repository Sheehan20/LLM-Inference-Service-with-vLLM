name: Performance Testing

on:
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      concurrency:
        description: 'Test concurrency level'
        required: false
        default: '20'
      requests:
        description: 'Number of requests'
        required: false
        default: '1000'
      model_name:
        description: 'Model to test'
        required: false
        default: 'microsoft/phi-2'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  performance-test:
    name: Performance Test
    runs-on: self-hosted  # Requires GPU runner
    if: github.repository_owner == 'your-org'  # Replace with actual org
    
    strategy:
      matrix:
        concurrency: [1, 5, 10, 20, 30]
        model: ['microsoft/phi-2']  # Add more models as needed
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install httpx asyncio pandas matplotlib
    
    - name: Pull latest image
      run: |
        docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
    
    - name: Start inference service
      run: |
        docker run -d \
          --name inference-service-test \
          --gpus all \
          -p 8000:8000 \
          -e MODEL_NAME=${{ matrix.model }} \
          -e CONCURRENCY_LIMIT=50 \
          -e LOG_LEVEL=WARNING \
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
    
    - name: Wait for service to be ready
      run: |
        timeout 300 bash -c 'until curl -f http://localhost:8000/healthz; do sleep 5; done'
    
    - name: Run performance test
      id: perf_test
      run: |
        python scripts/load_test.py \
          --url http://localhost:8000/v1/generate \
          --concurrency ${{ matrix.concurrency }} \
          --requests ${{ github.event.inputs.requests || '1000' }} \
          --prompt "Write a detailed explanation of machine learning" \
          --max-tokens 100 \
          --timeout 60 > perf_results.json
    
    - name: Parse results
      id: results
      run: |
        QPS=$(jq -r '.qps' perf_results.json)
        P95_LATENCY=$(jq -r '.latency_ms.p95' perf_results.json)
        ERROR_RATE=$(jq -r '.error_rate_percent' perf_results.json)
        
        echo "qps=$QPS" >> $GITHUB_OUTPUT
        echo "p95_latency=$P95_LATENCY" >> $GITHUB_OUTPUT
        echo "error_rate=$ERROR_RATE" >> $GITHUB_OUTPUT
    
    - name: Check performance thresholds
      run: |
        QPS=${{ steps.results.outputs.qps }}
        P95_LATENCY=${{ steps.results.outputs.p95_latency }}
        ERROR_RATE=${{ steps.results.outputs.error_rate }}
        
        # Define thresholds
        MIN_QPS=5
        MAX_P95_LATENCY=2000  # milliseconds
        MAX_ERROR_RATE=1      # percent
        
        if (( $(echo "$QPS < $MIN_QPS" | bc -l) )); then
          echo "❌ QPS too low: $QPS < $MIN_QPS"
          exit 1
        fi
        
        if (( $(echo "$P95_LATENCY > $MAX_P95_LATENCY" | bc -l) )); then
          echo "❌ P95 latency too high: $P95_LATENCY > $MAX_P95_LATENCY ms"
          exit 1
        fi
        
        if (( $(echo "$ERROR_RATE > $MAX_ERROR_RATE" | bc -l) )); then
          echo "❌ Error rate too high: $ERROR_RATE > $MAX_ERROR_RATE%"
          exit 1
        fi
        
        echo "✅ Performance test passed"
        echo "   QPS: $QPS"
        echo "   P95 Latency: ${P95_LATENCY}ms"
        echo "   Error Rate: ${ERROR_RATE}%"
    
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results-${{ matrix.model }}-${{ matrix.concurrency }}
        path: perf_results.json
    
    - name: Stop service
      if: always()
      run: |
        docker stop inference-service-test || true
        docker rm inference-service-test || true
    
    - name: GPU cleanup
      if: always()
      run: |
        nvidia-smi || true
        # Force cleanup GPU memory if needed

  benchmark-report:
    name: Generate Benchmark Report
    runs-on: ubuntu-latest
    needs: performance-test
    if: always()
    
    steps:
    - name: Download all results
      uses: actions/download-artifact@v3
      with:
        path: results/
    
    - name: Generate benchmark report
      run: |
        echo "# Performance Benchmark Report" > benchmark-report.md
        echo "Date: $(date)" >> benchmark-report.md
        echo "" >> benchmark-report.md
        
        for result in results/*/perf_results.json; do
          if [ -f "$result" ]; then
            echo "## $(basename $(dirname $result))" >> benchmark-report.md
            jq -r '. as $data | "- QPS: \($data.qps)\n- P95 Latency: \($data.latency_ms.p95)ms\n- Error Rate: \($data.error_rate_percent)%\n"' "$result" >> benchmark-report.md
          fi
        done
    
    - name: Upload benchmark report
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-report
        path: benchmark-report.md

  alert-on-failure:
    name: Alert on Performance Failure
    runs-on: ubuntu-latest
    needs: performance-test
    if: failure()
    
    steps:
    - name: Send alert
      run: |
        echo "🚨 Performance test failed for ${{ github.repository }}"
        # Add alerting logic (Slack webhook, email, etc.)
        # curl -X POST -H 'Content-type: application/json' \
        #   --data '{"text":"Performance test failed for ${{ github.repository }}"}' \
        #   ${{ secrets.SLACK_WEBHOOK_URL }}